{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f463273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "\n",
    "## 1. Dataset Preparation\n",
    "    1. Split\n",
    "    2. Transformations\n",
    "## 2. Training Techniques\n",
    "    1. Data Augmentation\n",
    "    2. Learning Rate Scheduling\n",
    "    3. Regularization\n",
    "        1. Weight Decay\n",
    "        2. Dropout\n",
    "        3. Batch Normalization\n",
    "## 3. ViT\n",
    "## 4. EfficientNet\n",
    "## 5. Performance\n",
    "## 6. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399cd84",
   "metadata": {},
   "source": [
    "# 1. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95236bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/adamlee/Downloads/Deepest/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd3c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to /Users/adamlee/Downloads/Deepest/dataset/stl10_binary.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████▏                 | 506658816/2640397119 [05:04<2:32:21, 233409.31it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "\n",
    "# Define the Mixup transform\n",
    "class MixupTransform:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        lam = torch.distributions.beta.Beta(self.alpha, self.alpha).sample().item()\n",
    "        batch_size = image.size()[0]\n",
    "        index = torch.randperm(batch_size)\n",
    "        mixed_image = lam * image + (1 - lam) * image[index, :]\n",
    "#         mixed_label = lam * label + (1 - lam) * label[index, :]\n",
    "        mixed_label = lam * label.unsqueeze(1) + (1 - lam) * label[index, :].unsqueeze(1)\n",
    "        return mixed_image, mixed_label\n",
    "\n",
    "# Define the transforms\n",
    "train_transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(96, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    MixupTransform(alpha=1.0)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "#     transforms.Resize(96),\n",
    "#     transforms.CenterCrop(96),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Load the dataset and apply the transforms\n",
    "train_set = STL10(root=data_path, split='test', download=True, transform=train_transform)\n",
    "test_set = STL10(root=data_path, split='train', download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9acf8b90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset STL10\n",
       "    Number of datapoints: 5000\n",
       "    Root location: /Users/adamlee/Downloads/Deepest/dataset\n",
       "    Split: train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "           )"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d201680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test set into test and validation sets\n",
    "num_test = len(test_set)\n",
    "indices = list(range(num_test))\n",
    "split = int(num_test * 0.8)\n",
    "test_idx, val_idx = indices[:split], indices[split:]\n",
    "testset = torch.utils.data.Subset(test_set, test_idx)\n",
    "valset = torch.utils.data.Subset(test_set, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed392d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for batching\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19e5e5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [ 0.9988,  1.0331,  1.0331,  ...,  1.0331,  1.0159,  1.0502],\n",
      "          [ 0.9646,  1.0159,  1.0159,  ...,  1.0673,  1.0502,  1.0331],\n",
      "          [ 0.9817,  0.9988,  1.0159,  ...,  1.0673,  1.0673,  1.0844]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [ 1.1681,  1.2031,  1.1856,  ...,  1.2381,  1.2206,  1.2381],\n",
      "          [ 1.1155,  1.1681,  1.1681,  ...,  1.2381,  1.2206,  1.2031],\n",
      "          [ 1.1331,  1.1506,  1.1681,  ...,  1.2206,  1.2206,  1.2381]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [ 1.3154,  1.3677,  1.3677,  ...,  1.4374,  1.4200,  1.4374],\n",
      "          [ 1.3328,  1.3677,  1.3677,  ...,  1.4374,  1.4200,  1.4200],\n",
      "          [ 1.3154,  1.3328,  1.3502,  ...,  1.4025,  1.4374,  1.4548]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0331,  1.0331,  1.0673,  ...,  0.6563,  0.7077,  0.7077],\n",
      "          [ 1.0502,  1.0159,  1.0331,  ...,  0.6563,  0.7077,  0.7248],\n",
      "          [ 1.0331,  0.9817,  0.9646,  ...,  0.6221,  0.7077,  0.7591],\n",
      "          ...,\n",
      "          [-1.7754, -1.7412, -1.7925,  ...,  0.9988,  0.9817,  0.9474],\n",
      "          [-1.8097, -1.7412, -1.7754,  ...,  1.0331,  1.0159,  0.9817],\n",
      "          [-1.7412, -1.7754, -1.7925,  ...,  1.0331,  0.9988,  0.9646]],\n",
      "\n",
      "         [[ 1.5182,  1.5007,  1.5007,  ...,  1.1506,  1.2031,  1.2206],\n",
      "          [ 1.5007,  1.5182,  1.5182,  ...,  1.1155,  1.1331,  1.1681],\n",
      "          [ 1.4832,  1.5357,  1.5007,  ...,  1.0630,  1.0630,  1.1155],\n",
      "          ...,\n",
      "          [-1.7206, -1.6506, -1.6856,  ...,  1.5182,  1.5532,  1.5357],\n",
      "          [-1.6681, -1.6681, -1.6856,  ...,  1.5357,  1.5532,  1.5532],\n",
      "          [-1.6856, -1.6506, -1.6681,  ...,  1.5707,  1.5357,  1.5707]],\n",
      "\n",
      "         [[ 1.9777,  1.9603,  2.0648,  ...,  1.7685,  1.8208,  1.8208],\n",
      "          [ 1.9951,  2.0125,  2.0300,  ...,  1.6640,  1.6988,  1.7511],\n",
      "          [ 1.9951,  2.0474,  2.0997,  ...,  1.5245,  1.6465,  1.7163],\n",
      "          ...,\n",
      "          [-1.4907, -1.4384, -1.4210,  ...,  2.1520,  2.1694,  2.1694],\n",
      "          [-1.4733, -1.4559, -1.4384,  ...,  2.1694,  2.1694,  2.1694],\n",
      "          [-1.4036, -1.4036, -1.4384,  ...,  2.1868,  2.1868,  2.2217]]],\n",
      "\n",
      "\n",
      "        [[[-0.6452, -0.6452, -0.6452,  ..., -0.7993, -0.7822, -0.7479],\n",
      "          [-0.5596, -0.5596, -0.5596,  ..., -0.8164, -0.7479, -0.6794],\n",
      "          [-0.6281, -0.6281, -0.6281,  ..., -0.8164, -0.7308, -0.6281],\n",
      "          ...,\n",
      "          [-1.0048, -1.1418, -1.1932,  ..., -1.0904, -1.0904, -1.1247],\n",
      "          [-1.0048, -1.1418, -1.1932,  ..., -1.0390, -1.0390, -1.0733],\n",
      "          [-1.0048, -1.1418, -1.1932,  ..., -1.0219, -1.0390, -1.0733]],\n",
      "\n",
      "         [[-0.1450, -0.1450, -0.1450,  ..., -0.2325, -0.2150, -0.1800],\n",
      "          [-0.0574, -0.0574, -0.0574,  ..., -0.2500, -0.1800, -0.1099],\n",
      "          [-0.1275, -0.1275, -0.1275,  ..., -0.2500, -0.1625, -0.0574],\n",
      "          ...,\n",
      "          [-0.4251, -0.5651, -0.6176,  ..., -0.6001, -0.6001, -0.6352],\n",
      "          [-0.4251, -0.5651, -0.6176,  ..., -0.5476, -0.5476, -0.5826],\n",
      "          [-0.4251, -0.5651, -0.6176,  ..., -0.5301, -0.5476, -0.5826]],\n",
      "\n",
      "         [[ 0.7228,  0.7228,  0.7228,  ...,  0.6008,  0.6182,  0.6531],\n",
      "          [ 0.8099,  0.8099,  0.8099,  ...,  0.5834,  0.6531,  0.7228],\n",
      "          [ 0.7402,  0.7402,  0.7402,  ...,  0.5834,  0.6705,  0.7751],\n",
      "          ...,\n",
      "          [-0.7413, -0.8807, -0.9330,  ..., -1.0027, -1.0027, -1.0376],\n",
      "          [-0.7413, -0.8807, -0.9330,  ..., -0.9504, -0.9504, -0.9853],\n",
      "          [-0.7413, -0.8807, -0.9330,  ..., -0.9330, -0.9504, -0.9853]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.4954,  1.5125,  1.5125,  ...,  1.5468,  1.5297,  1.5297],\n",
      "          [ 1.4954,  1.4954,  1.4783,  ...,  1.5468,  1.5297,  1.5297],\n",
      "          [ 1.4783,  1.4954,  1.4783,  ...,  1.5297,  1.5297,  1.5297],\n",
      "          ...,\n",
      "          [-0.2513, -0.2513, -0.2513,  ..., -0.0458, -0.0972, -0.1486],\n",
      "          [-0.2513, -0.3027, -0.2513,  ..., -0.1657, -0.0972,  0.0569],\n",
      "          [-0.2171, -0.2342, -0.3198,  ..., -0.1314, -0.1314, -0.1828]],\n",
      "\n",
      "         [[ 1.6232,  1.6408,  1.6408,  ...,  1.6758,  1.6583,  1.6583],\n",
      "          [ 1.6232,  1.6232,  1.6057,  ...,  1.6758,  1.6583,  1.6583],\n",
      "          [ 1.6057,  1.6232,  1.6057,  ...,  1.6583,  1.6583,  1.6583],\n",
      "          ...,\n",
      "          [-0.0224, -0.0224, -0.0399,  ...,  0.3102,  0.2402,  0.2052],\n",
      "          [-0.0224, -0.0749, -0.0399,  ...,  0.0826,  0.1352,  0.3102],\n",
      "          [ 0.0301,  0.0826,  0.0126,  ...,  0.0651,  0.0826,  0.0651]],\n",
      "\n",
      "         [[ 2.2566,  2.2740,  2.2740,  ...,  2.2740,  2.2566,  2.2566],\n",
      "          [ 2.2566,  2.2566,  2.2391,  ...,  2.2740,  2.2566,  2.2566],\n",
      "          [ 2.2391,  2.2566,  2.2391,  ...,  2.2566,  2.2566,  2.2566],\n",
      "          ...,\n",
      "          [ 0.5659,  0.6531,  0.6356,  ...,  1.1062,  1.1237,  1.0539],\n",
      "          [ 0.7576,  0.6008,  0.6356,  ...,  0.8448,  0.9319,  1.1237],\n",
      "          [ 0.7576,  0.8099,  0.7576,  ...,  0.7576,  0.8448,  0.8274]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1187,  1.1187,  1.1187,  ...,  1.0502,  1.0502,  1.0502],\n",
      "          [ 1.1358,  1.1358,  1.1358,  ...,  1.0673,  1.0673,  1.0844],\n",
      "          [ 1.1700,  1.1529,  1.1700,  ...,  1.0844,  1.0673,  1.0673],\n",
      "          ...,\n",
      "          [ 0.6563,  0.7591,  0.3823,  ...,  0.2282, -0.2342,  0.1768],\n",
      "          [ 0.5536,  0.2111,  0.2282,  ...,  0.2453,  0.1426,  0.1597],\n",
      "          [ 0.0912,  0.1768,  0.0056,  ...,  0.3481,  0.1426,  0.1768]],\n",
      "\n",
      "         [[ 1.9909,  1.9909,  1.9909,  ...,  1.8859,  1.8859,  1.8859],\n",
      "          [ 2.0084,  2.0084,  2.0084,  ...,  1.9209,  1.9034,  1.9209],\n",
      "          [ 2.0084,  2.0084,  2.0084,  ...,  1.9209,  1.9384,  1.9384],\n",
      "          ...,\n",
      "          [ 0.2577,  0.3803,  0.1176,  ..., -0.1099, -0.5651, -0.1099],\n",
      "          [ 0.2052, -0.1450, -0.0049,  ..., -0.0924, -0.1625, -0.0924],\n",
      "          [-0.3025, -0.1625, -0.2850,  ..., -0.0049, -0.2150, -0.1275]],\n",
      "\n",
      "         [[ 2.5180,  2.5180,  2.5180,  ...,  2.3960,  2.3960,  2.3960],\n",
      "          [ 2.5354,  2.5354,  2.5354,  ...,  2.3611,  2.3960,  2.4308],\n",
      "          [ 2.5354,  2.5180,  2.5354,  ...,  2.3786,  2.3960,  2.4134],\n",
      "          ...,\n",
      "          [-0.0615,  0.0082, -0.3404,  ..., -0.3404, -0.7761, -0.4450],\n",
      "          [-0.0615, -0.3927, -0.3927,  ..., -0.3927, -0.4450, -0.5670],\n",
      "          [-0.5495, -0.4101, -0.6018,  ..., -0.3753, -0.6367, -0.6367]]],\n",
      "\n",
      "\n",
      "        [[[-1.4158, -1.2103, -1.2445,  ..., -0.8164, -0.8164, -0.7137],\n",
      "          [-1.2788, -1.2617, -1.3644,  ..., -0.9363, -0.6109, -0.7650],\n",
      "          [-1.2445, -1.3644, -1.3473,  ..., -0.7650, -0.6623, -1.0562],\n",
      "          ...,\n",
      "          [-0.5082,  1.1872,  0.7077,  ...,  1.4098,  1.1529,  0.9646],\n",
      "          [ 0.0056,  0.1426,  0.1254,  ..., -0.0116, -0.5767, -0.7822],\n",
      "          [-0.0116, -0.1143, -0.1143,  ..., -1.0733, -0.9363, -0.8678]],\n",
      "\n",
      "         [[-1.2304, -1.0903, -1.0903,  ..., -0.6001, -0.6001, -0.5651],\n",
      "          [-1.0903, -1.0728, -1.1429,  ..., -0.7052, -0.4776, -0.6527],\n",
      "          [-1.0728, -1.2304, -1.1954,  ..., -0.6176, -0.5301, -0.9678],\n",
      "          ...,\n",
      "          [-0.2850,  1.2206,  0.7829,  ...,  1.6408,  1.4307,  1.2381],\n",
      "          [ 0.3803,  0.5553,  0.4503,  ...,  0.1877, -0.3901, -0.6176],\n",
      "          [ 0.3978,  0.4328,  0.3452,  ..., -1.0728, -0.9503, -0.8277]],\n",
      "\n",
      "         [[-1.0898, -0.9504, -1.1247,  ..., -0.9853, -0.8284, -0.5321],\n",
      "          [-1.1073, -1.0027, -1.1944,  ..., -1.1421, -0.7587, -0.6018],\n",
      "          [-1.3164, -1.2467, -1.1944,  ..., -0.9678, -0.7238, -0.8981],\n",
      "          ...,\n",
      "          [-0.0790,  1.1062,  0.6531,  ...,  1.5071,  1.4548,  1.2457],\n",
      "          [ 0.3916,  0.2173,  0.0605,  ...,  0.2348, -0.3753, -0.5844],\n",
      "          [ 0.3568,  0.0605, -0.0267,  ..., -1.0898, -0.9330, -0.7936]]]]), tensor([8, 1, 1, 6, 3, 1, 3, 9, 4, 3, 5, 9, 8, 2, 0, 5, 9, 2, 0, 3, 5, 5, 6, 6,\n",
      "        3, 8, 0, 3, 8, 0, 6, 7])]\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(val_loader))\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b05a9c",
   "metadata": {},
   "source": [
    "# 2. ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592539a8",
   "metadata": {},
   "source": [
    "ViT-L/16\n",
    "\n",
    "ViT (Samll)\n",
    "layers: 4\n",
    "hidden size: 256\n",
    "heads: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "214a3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_classes=10, img_size=96, patch_size=16, num_channels=3, emb_dim=256, num_heads=4, num_layers=4):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.patch_embed = nn.Conv2d(num_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "\n",
    "        self.position_embed = nn.Parameter(torch.zeros(1, 1 + self.num_patches, emb_dim))\n",
    "        self.position_dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = x + self.position_embed\n",
    "        x = self.position_dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b2367",
   "metadata": {},
   "source": [
    "# 3. EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd401df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Define the model\n",
    "num_classes = 10\n",
    "efficient_net = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "efficient_net.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = efficient_net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, loss: {running_loss / len(trainloader):.4f}\")\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = efficient_net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81370154",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a29d3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d44c4f21",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     15\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     17\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/datasets/stl10.py:121\u001b[0m, in \u001b[0;36mSTL10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    118\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39mtranspose(img, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'label'"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_acc += (preds == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_acc += (preds == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc /= len(valset)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc * 100:.2f}%\")\n",
    "    print(f\"Validataion Loss: {val_loss:.4f}, Validataion Acc: {val_acc * 100:.2f}%\")\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/validation', val_acc, epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80786c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_acc += (preds == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_acc /= len(test_dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9b456",
   "metadata": {},
   "source": [
    "# 4. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6089e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
